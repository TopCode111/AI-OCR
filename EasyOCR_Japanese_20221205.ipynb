{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4n-6SP0lmZQP"
      },
      "source": [
        "#AI - OCR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PcQGJLDmfJi"
      },
      "source": [
        "### Required package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4LonUuYe7Hg",
        "outputId": "b8e2f810-3d9d-438e-f573-1e9981fee579"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/aziz-ullah-khan/EasyOCR.git  \n",
        "#https://github.com/TopCode111/AI-OCR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAqr0plOfWsN"
      },
      "source": [
        "### Importing required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "No4sBGsswp3K",
        "outputId": "a3db6889-ec19-4d9e-a1ac-2d09301c3db0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "f:\\Git\\AI-OCR\\trainer\n"
          ]
        }
      ],
      "source": [
        "#change directory to import train file accessories\n",
        "%cd f:/Git/AI-OCR/trainer     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uMvxHWt3InJ3"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch.backends.cudnn as cudnn\n",
        "import yaml\n",
        "from train import train\n",
        "from utils import AttrDict\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S149dL1afaw_"
      },
      "source": [
        "### Preparing training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fHdc0_CZxMYx"
      },
      "outputs": [],
      "source": [
        "# create directories for training and evaluation\n",
        "%mkdir 'f:/Git/AI-OCR/trainer/all_data/ja_train_filtered/'\n",
        "%mkdir 'f:/Git/AI-OCR/trainer/all_data/ja_val/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9pu9E4QHIy55"
      },
      "outputs": [],
      "source": [
        "BASE_DIR = 'f:/Git/AI-OCR/1moji-ground-truth/'       # Base directory of the unzip folder\n",
        "target_dir_training = 'f:/Git/AI-OCR/trainer/all_data/ja_train_filtered/'\n",
        "target_dir_val = 'f:/Git/AI-OCR/trainer/all_data/ja_val/'\n",
        "training_images_names = []     \n",
        "training_text_data = []\n",
        "\n",
        "training_files = os.listdir(BASE_DIR)\n",
        "\n",
        "for training_file in training_files:\n",
        "  if training_file.endswith(('.jpg', '.png', 'jpeg', 'tif')):\n",
        "    image_name_no_extension = training_file.split('.')[0]\n",
        "    training_text_file = image_name_no_extension + '.gt.txt'\n",
        "    training_text_file_directory = BASE_DIR + training_text_file\n",
        "   # print(training_text_file_directory)\n",
        "    if os.path.isfile(training_text_file_directory):\n",
        "      with open(training_text_file_directory) as f:\n",
        "        training_images_names.append(training_file)\n",
        "        training_text_data.append(f.read())\n",
        "    else:\n",
        "      pass\n",
        "      #print (\"text File does not exist!\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJ7qVwQWAW_U"
      },
      "source": [
        "Training and Validation Data Preparing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAQcmUucI8ED",
        "outputId": "0c2ca7cc-7a8c-4b8f-bb96-8edcdea30d94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total files:  455962\n",
            "Total training files:  410365\n",
            "Total val files:  45597\n"
          ]
        }
      ],
      "source": [
        "data = pd.DataFrame()\n",
        "data['filename'] = training_images_names\n",
        "data['words'] = training_text_data\n",
        "\n",
        "training_data, validation_data = train_test_split(data, test_size=0.1)\n",
        "\n",
        "print('Total files: ', len(training_images_names))\n",
        "print('Total training files: ', len(training_data))\n",
        "print('Total val files: ', len(validation_data))\n",
        "\n",
        "\n",
        "\n",
        "for file in training_data['filename']:\n",
        "  shutil.copy2(os.path.join(BASE_DIR, file), target_dir_training)\n",
        "\n",
        "for file in validation_data['filename']:\n",
        "  shutil.copy2(os.path.join(BASE_DIR, file), target_dir_val)\n",
        "\n",
        "# Save the training data to csv\n",
        "training_data.to_csv(target_dir_training + 'labels.csv', encoding = 'utf-8', index = False)\n",
        "validation_data.to_csv(target_dir_val + 'labels.csv', encoding = 'utf-8', index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MLHqAalmn71"
      },
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "l7gmI1gRrdC5"
      },
      "outputs": [],
      "source": [
        "cudnn.benchmark = True\n",
        "cudnn.deterministic = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "u1MstfmPsj8a"
      },
      "outputs": [],
      "source": [
        "def get_config(file_path):\n",
        "    with open(file_path, 'r', encoding=\"utf8\") as stream:\n",
        "        opt = yaml.safe_load(stream)\n",
        "    opt = AttrDict(opt)\n",
        "    if opt.lang_char == 'None':\n",
        "        characters = ''\n",
        "        for data in opt['select_data'].split('-'):\n",
        "            csv_path = os.path.join(opt['train_data'], data, 'labels.csv')\n",
        "            df = pd.read_csv(csv_path, sep=',', engine='python', usecols=['filename', 'words'], keep_default_na=False)\n",
        "            all_char = ''.join(df['words'])\n",
        "            characters += ''.join(set(all_char))\n",
        "        characters = sorted(set(characters))\n",
        "        opt.character= ''.join(characters)\n",
        "    else:\n",
        "        opt.character = opt.number + opt.symbol + opt.lang_char\n",
        "    os.makedirs(f'./saved_models/{opt.experiment_name}', exist_ok=True)\n",
        "    return opt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lI6MkKWttSGl",
        "outputId": "6ff6bd0d-b31a-4125-bb8f-e5b0708d29e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtering the images containing characters which are not in opt.character\n",
            "Filtering the images whose label is longer than opt.batch_max_length\n",
            "--------------------------------------------------------------------------------\n",
            "dataset_root: all_data\n",
            "opt.select_data: ['ja_train_filtered']\n",
            "opt.batch_ratio: ['1']\n",
            "--------------------------------------------------------------------------------\n",
            "dataset_root:    all_data\t dataset: ja_train_filtered\n",
            "all_data/ja_train_filtered\n",
            "sub-directory:\t/ja_train_filtered\t num samples: 2\n",
            "num total samples of ja_train_filtered: 2 x 1.0 (total_data_usage_ratio) = 2\n",
            "num samples of ja_train_filtered per batch: 1 x 1.0 (batch_ratio) = 1\n",
            "--------------------------------------------------------------------------------\n",
            "Total_batch_size: 1 = 1\n",
            "--------------------------------------------------------------------------------\n",
            "dataset_root:    all_data/ja_val\t dataset: /\n",
            "all_data/ja_val/\n",
            "sub-directory:\t/.\t num samples: 2\n",
            "--------------------------------------------------------------------------------\n",
            "No Transformation module specified\n",
            "model input parameters 64 600 20 1 256 256 2287 34 None VGG BiLSTM CTC\n",
            "Model:\n",
            "DataParallel(\n",
            "  (module): Model(\n",
            "    (FeatureExtraction): VGG_FeatureExtractor(\n",
            "      (ConvNet): Sequential(\n",
            "        (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "        (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (4): ReLU(inplace=True)\n",
            "        (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "        (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (7): ReLU(inplace=True)\n",
            "        (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (9): ReLU(inplace=True)\n",
            "        (10): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
            "        (11): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (13): ReLU(inplace=True)\n",
            "        (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (16): ReLU(inplace=True)\n",
            "        (17): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
            "        (18): Conv2d(256, 256, kernel_size=(2, 2), stride=(1, 1))\n",
            "        (19): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (AdaptiveAvgPool): AdaptiveAvgPool2d(output_size=(None, 1))\n",
            "    (SequenceModeling): Sequential(\n",
            "      (0): BidirectionalLSTM(\n",
            "        (rnn): LSTM(256, 256, batch_first=True, bidirectional=True)\n",
            "        (linear): Linear(in_features=512, out_features=256, bias=True)\n",
            "      )\n",
            "      (1): BidirectionalLSTM(\n",
            "        (rnn): LSTM(256, 256, batch_first=True, bidirectional=True)\n",
            "        (linear): Linear(in_features=512, out_features=256, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (Prediction): Linear(in_features=256, out_features=2287, bias=True)\n",
            "  )\n",
            ")\n",
            "Modules, Parameters\n",
            "module.FeatureExtraction.ConvNet.0.weight 288\n",
            "module.FeatureExtraction.ConvNet.0.bias 32\n",
            "module.FeatureExtraction.ConvNet.3.weight 18432\n",
            "module.FeatureExtraction.ConvNet.3.bias 64\n",
            "module.FeatureExtraction.ConvNet.6.weight 73728\n",
            "module.FeatureExtraction.ConvNet.6.bias 128\n",
            "module.FeatureExtraction.ConvNet.8.weight 147456\n",
            "module.FeatureExtraction.ConvNet.8.bias 128\n",
            "module.FeatureExtraction.ConvNet.11.weight 294912\n",
            "module.FeatureExtraction.ConvNet.12.weight 256\n",
            "module.FeatureExtraction.ConvNet.12.bias 256\n",
            "module.FeatureExtraction.ConvNet.14.weight 589824\n",
            "module.FeatureExtraction.ConvNet.15.weight 256\n",
            "module.FeatureExtraction.ConvNet.15.bias 256\n",
            "module.FeatureExtraction.ConvNet.18.weight 262144\n",
            "module.FeatureExtraction.ConvNet.18.bias 256\n",
            "module.SequenceModeling.0.rnn.weight_ih_l0 262144\n",
            "module.SequenceModeling.0.rnn.weight_hh_l0 262144\n",
            "module.SequenceModeling.0.rnn.bias_ih_l0 1024\n",
            "module.SequenceModeling.0.rnn.bias_hh_l0 1024\n",
            "module.SequenceModeling.0.rnn.weight_ih_l0_reverse 262144\n",
            "module.SequenceModeling.0.rnn.weight_hh_l0_reverse 262144\n",
            "module.SequenceModeling.0.rnn.bias_ih_l0_reverse 1024\n",
            "module.SequenceModeling.0.rnn.bias_hh_l0_reverse 1024\n",
            "module.SequenceModeling.0.linear.weight 131072\n",
            "module.SequenceModeling.0.linear.bias 256\n",
            "module.SequenceModeling.1.rnn.weight_ih_l0 262144\n",
            "module.SequenceModeling.1.rnn.weight_hh_l0 262144\n",
            "module.SequenceModeling.1.rnn.bias_ih_l0 1024\n",
            "module.SequenceModeling.1.rnn.bias_hh_l0 1024\n",
            "module.SequenceModeling.1.rnn.weight_ih_l0_reverse 262144\n",
            "module.SequenceModeling.1.rnn.weight_hh_l0_reverse 262144\n",
            "module.SequenceModeling.1.rnn.bias_ih_l0_reverse 1024\n",
            "module.SequenceModeling.1.rnn.bias_hh_l0_reverse 1024\n",
            "module.SequenceModeling.1.linear.weight 131072\n",
            "module.SequenceModeling.1.linear.bias 256\n",
            "module.Prediction.weight 585472\n",
            "module.Prediction.bias 2287\n",
            "Total Trainable Params: 4344175\n",
            "Trainable params num :  4344175\n",
            "Optimizer:\n",
            "Adadelta (\n",
            "Parameter Group 0\n",
            "    eps: 1e-08\n",
            "    lr: 1.0\n",
            "    rho: 0.95\n",
            "    weight_decay: 0\n",
            ")\n",
            "------------ Options -------------\n",
            "number: 0123456789\n",
            "symbol:  !\"#$%&'()*+,-./0123456789:;<=>?@[\\]^_`{|}~‥…※♪、。々〈〉《》「」『』【】〔〕\n",
            "lang_char:  !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~‥…※♪、。々〈〉《》「」『』【】〔〕あぃいうぇえおかがきぎくぐけげこごさざしじすずせぜそぞただちぢっつづてでとどなにぬねのはばぱひびぴふぶぷへべぺほぼぽまみむめもゃやゅゆょよらりるれろわをんァアィイゥウェエォオカガキギクグケゲコゴサザシジスズセゼソゾタダチヂッツヅテデトドナニヌネノハバパヒビピフブプヘベペホボポマミムメモャヤュユョヨラリルレロワヱヲンヴヶ・ー一丁七万丈三上下不与丑世丘丙丞両並中串丸丹主乃久之乗乙九也乱乳乾亀了予争事二互五井亘亜亡交亥亨享京亭亮人仁今介仏仕他付仙代令以仮仰仲件任企伊伍伎伏休会伝伯伴伶伸似伽位低住佐佑体何余作佳使例侍侑供依価便係俊俗保信俣修俵倉個倍倒倖候借値倫倭倶偉偏健側偵偽傍傑備債傷働像僧儀優允元兄充先光克免兎児党入全八公六共兵其具典兼内円再写冠冨冬冴冶冷凍凡処凪出刀刃分切刈刑列初判別利制刷券刺刻則削前剛剣剤剥副割創劇力功加助労効勅勇勉動勘務勝勢勤勧勲包化北匠匡区医十千升午半卒卓協南単博占卯印危即卵卸厚原厨厳去参又叉及友双反収取受叡口古句只叫召可台史右叶号司各合吉同名后吏吐向君吟否吸吹吾呂呉告呑周味呼命和咲哀品哉員哲唄唐唯唱商問啓善喜喬喰営嗣嘉噌器四回因団囲図固国國園土圧在圭地坂均坊坐坪垂型垢垣埋城埜域執基埼堀堂堅堤堰報場堺塔塗塚塩塵境墓増墨墳壁壇壊士壬壮声壱売壷変夏夕外多夜夢大天太夫央失夷奇奈奉奏契奥奨女奴好如妃妙妹妻姉始姓委姥姫姿威娘婆婚婦嫌嬉子孔字存孝孟季孤学孫宅宇守安完宏宗官宙定宜宝実客宣室宥宮宰害家容宿寂寄寅密富寒寛寝察寧審寸寺対寿封専射将尊尋導小少尚尭就尺尻尼尽尾尿局居屈屋展属層屯山岐岡岩岬岱岳岸峠峡峨峯峰島峻崇崎崩嵐嵩嵯嶋嶺巌川州巡巣工左巧巨差己巳巴巻巾市布帆希帝師席帯帰帳常帽幅幌幕幡幣干平年幸幹幽幾庁広庄床序底店府度座庫庭庵康庸廃廉廣延建廻弁式弐弓引弘弟弥弦弱張強弾当形彦彩彪彫彬彰影役彼往征径待律後徒従得御復微徳徹心必忌忍志応忠快念怒怜思急性恋恐恒恩恭息恵悌悟悠患悦悪悲情惇惑惟惣想意愚愛感慈態慎慣慧慶憂憲憶懐懸戎成我戒戦戯戸戻房所扇手才打払扶承技投抗折抜抱押担拓拝拡拳拾持指挙振捕捨捷掃排掘掛採探接推掻提揖揚換揮援揺損摂摘摩摺撃撫播撮操擦擬支改攻放政故敏救敗教敢散敦敬数整敵敷文斉斎斐斑斗料斜斤断斯新方於施旅旋族旗日旦旧旨早旬旭旺昂昆昇昌明易星映春昭是昼時晃晋晩普景晴晶智暁暖暗暢暦暮暴曇曙曜曲曳更書曹曽曾替最月有朋服朔朗望朝期木未末本札朱朴杉李杏材村杖杜束条来杭東杵松板析枕林枚果枝枯架柄柊柏柑染柔柚柱柳柴査柿栃栄栖栗校株核根格桂桃案桐桑桜桝桧桶梁梅梓梢梨梯械梶棄棒棚棟森椋植椎検椿楊楓楠楢業楯極楼楽榊榎榛構槌様槙槻樋標模権横樫樹樺樽橋橘機檀櫛欠次欣欧欲欽歌歓止正此武歩歯歳歴死殊残殖段殺殻殿毅母毎毒比毛氏民気水氷永汀汁求汐汗汚江池汰汲決沈沓沖沙沢河油治沼泉泊法波泣泥注泰洋洗洞津洪洲活派流浄浅浜浦浩浪浮浴海消涌涙液涼淀淑淡深淳淵混添清済渉渋渓渕渚減渡渥温測港湊湖湧湯湾湿満源準溜溝溶滅滋滑滝漁漆漏演漢漬潔潜潟潤潮潰澄澤激濃濱瀧瀬灘火灯灰災炉炎炭点為烈烏無焦然焼煙照煮熊熟熱燃燈燕燦燭爆爪父爽爾片版牛牟牧物特犬犯状狂狐狗狩独狭狼猛猪猫献猿獄獅獣玄率玉王玖玲珍珠現球理琉琢琳琴瑚瑛瑞瑠瑳璃環瓜瓦瓶甘甚生産用甫田由甲申男町画界畑畔留畜畝畠略番異畳疾病症痛療発登白百的皆皇皮皿盆益盗盛盟監盤目盲直相省眉看県眞真眠眼着督睦瞬瞳矢知矩短石砂研砲破硫硬碑碧碩確磁磐磨磯礁示礼社祇祈祉祐祖祝神祢祥票祭禁禄禅禎福禰秀私秋科秘秦秩称移稀程税稔稗稚種稲穂積穏穴究空突窓窪立竜章童竪端競竹笑笛笠符第笹筆等筋筑筒答策箇箕算管箱箸節範築篠篤篭簡簾籍米粉粒粕粗粟粥精糖糞糠糸系紀約紅紋納純紗紘紙級素紡索紫細紳紹紺終組経結絡絢給統絵絶絹継続綜維綱網綾綿緋総緑緒線締編緩練縁縄縦縫縮績繁織繰罪置羅羊美群義羽翁習翔翠翼耀老考者耐耕耳耶聖聞聡聴職肇肉肌肝股肥肩育肺背胞胡胤胸能脂脇脈脚脱脳腐腕腫腰腸腹膜膳臣臥臨自臭至致臼興舌舎舘舛舜舞舟航般船艦良色艶芋芙芝芥芦花芳芸芹芽苅苑苔苗若苦苫英茂茄茅茉茜茨茶草荒荘荷荻莉菅菊菌菓菖菜華菱萌萩萱落葉葛葦葵蒔蒲蒸蒼蓋蓑蓬蓮蔦蔭蔵蕗薄薩薫薬薮藁藍藤藻蘇蘭虎虚虫虹虻蚊蛇蛍蛭蜂蜜蝦蝶融螺蟹蟻血衆行術街衛衝衡衣表袋袖被裁裂装裏裕補裟裸製複西要覆覇見規視覚覧親観角解触言計訓託記訪設許訳訴診証評詞詠試詩詰話誉誌認誓誘語誠誤説読課調談請諏論諭諸謙講謝謹識警議譲護讃谷豆豊豚象豪貝貞負財貢貧貨販貫責貯貴買貸費賀賃資賞賢質赤赦走起超越足跡路跳踏身車軌軍軒軟転軸軽載輔輝輪輸辛辞辰農辺辻込迎近返迦迫述迷追退送逃逆透途通速造逢連週進逸遅遊運過道達違遠遣遥適選遺遼避邑那邦邪郁郎郡部郭郵郷都配酒酔酢酸醍醐采釈里重野量金釘釜針釣鈴鉄鉛鉢鉱鉾銀銃銅銘銭鋭鋼錦録鍋鍛鍬鍵鎌鎖鎮鏡鐘鑑長門閉開閑間関閣闇闘阪防阿陀附降限院陣除陰陳陵陶陸険陽隅隆隈隊階随隔際障隠隣隼雀雁雄雅集雑雛離難雨雪雲零雷電震霊霜霞霧露青靖静非面革鞍鞠韓音響頂頃項順須預頓領頭頼題額顔顕願類風飛食飯飲飼飽飾餅養館首香馨馬駄駅駆駐駒駿騎験骨高髪鬼魁魂魅魔魚鮎鮫鮮鯉鯨鳥鳩鳳鳴鴨鴻鵜鶏鶴鷲鷹鷺鹿麓麗麦麻麿黄黒黙鼓鼠鼻齢龍＊\n",
            "experiment_name: ja_filtered\n",
            "train_data: all_data\n",
            "valid_data: all_data/ja_val\n",
            "manualSeed: 1111\n",
            "workers: 0\n",
            "batch_size: 1\n",
            "prefetch_factor: 2\n",
            "num_iter: 30\n",
            "valInterval: 10\n",
            "saved_model: \n",
            "FT: False\n",
            "optim: False\n",
            "lr: 1.0\n",
            "beta1: 0.9\n",
            "rho: 0.95\n",
            "eps: 1e-08\n",
            "grad_clip: 5\n",
            "select_data: ['ja_train_filtered']\n",
            "batch_ratio: ['1']\n",
            "total_data_usage_ratio: 1.0\n",
            "batch_max_length: 34\n",
            "imgH: 64\n",
            "imgW: 600\n",
            "rgb: False\n",
            "contrast_adjust: 0.0\n",
            "sensitive: True\n",
            "PAD: True\n",
            "data_filtering_off: False\n",
            "Transformation: None\n",
            "FeatureExtraction: VGG\n",
            "SequenceModeling: BiLSTM\n",
            "Prediction: CTC\n",
            "num_fiducial: 20\n",
            "input_channel: 1\n",
            "output_channel: 256\n",
            "hidden_size: 256\n",
            "decode: greedy\n",
            "new_prediction: False\n",
            "freeze_FeatureFxtraction: False\n",
            "freeze_SequenceModeling: False\n",
            "character: 0123456789 !\"#$%&'()*+,-./0123456789:;<=>?@[\\]^_`{|}~‥…※♪、。々〈〉《》「」『』【】〔〕 !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~‥…※♪、。々〈〉《》「」『』【】〔〕あぃいうぇえおかがきぎくぐけげこごさざしじすずせぜそぞただちぢっつづてでとどなにぬねのはばぱひびぴふぶぷへべぺほぼぽまみむめもゃやゅゆょよらりるれろわをんァアィイゥウェエォオカガキギクグケゲコゴサザシジスズセゼソゾタダチヂッツヅテデトドナニヌネノハバパヒビピフブプヘベペホボポマミムメモャヤュユョヨラリルレロワヱヲンヴヶ・ー一丁七万丈三上下不与丑世丘丙丞両並中串丸丹主乃久之乗乙九也乱乳乾亀了予争事二互五井亘亜亡交亥亨享京亭亮人仁今介仏仕他付仙代令以仮仰仲件任企伊伍伎伏休会伝伯伴伶伸似伽位低住佐佑体何余作佳使例侍侑供依価便係俊俗保信俣修俵倉個倍倒倖候借値倫倭倶偉偏健側偵偽傍傑備債傷働像僧儀優允元兄充先光克免兎児党入全八公六共兵其具典兼内円再写冠冨冬冴冶冷凍凡処凪出刀刃分切刈刑列初判別利制刷券刺刻則削前剛剣剤剥副割創劇力功加助労効勅勇勉動勘務勝勢勤勧勲包化北匠匡区医十千升午半卒卓協南単博占卯印危即卵卸厚原厨厳去参又叉及友双反収取受叡口古句只叫召可台史右叶号司各合吉同名后吏吐向君吟否吸吹吾呂呉告呑周味呼命和咲哀品哉員哲唄唐唯唱商問啓善喜喬喰営嗣嘉噌器四回因団囲図固国國園土圧在圭地坂均坊坐坪垂型垢垣埋城埜域執基埼堀堂堅堤堰報場堺塔塗塚塩塵境墓増墨墳壁壇壊士壬壮声壱売壷変夏夕外多夜夢大天太夫央失夷奇奈奉奏契奥奨女奴好如妃妙妹妻姉始姓委姥姫姿威娘婆婚婦嫌嬉子孔字存孝孟季孤学孫宅宇守安完宏宗官宙定宜宝実客宣室宥宮宰害家容宿寂寄寅密富寒寛寝察寧審寸寺対寿封専射将尊尋導小少尚尭就尺尻尼尽尾尿局居屈屋展属層屯山岐岡岩岬岱岳岸峠峡峨峯峰島峻崇崎崩嵐嵩嵯嶋嶺巌川州巡巣工左巧巨差己巳巴巻巾市布帆希帝師席帯帰帳常帽幅幌幕幡幣干平年幸幹幽幾庁広庄床序底店府度座庫庭庵康庸廃廉廣延建廻弁式弐弓引弘弟弥弦弱張強弾当形彦彩彪彫彬彰影役彼往征径待律後徒従得御復微徳徹心必忌忍志応忠快念怒怜思急性恋恐恒恩恭息恵悌悟悠患悦悪悲情惇惑惟惣想意愚愛感慈態慎慣慧慶憂憲憶懐懸戎成我戒戦戯戸戻房所扇手才打払扶承技投抗折抜抱押担拓拝拡拳拾持指挙振捕捨捷掃排掘掛採探接推掻提揖揚換揮援揺損摂摘摩摺撃撫播撮操擦擬支改攻放政故敏救敗教敢散敦敬数整敵敷文斉斎斐斑斗料斜斤断斯新方於施旅旋族旗日旦旧旨早旬旭旺昂昆昇昌明易星映春昭是昼時晃晋晩普景晴晶智暁暖暗暢暦暮暴曇曙曜曲曳更書曹曽曾替最月有朋服朔朗望朝期木未末本札朱朴杉李杏材村杖杜束条来杭東杵松板析枕林枚果枝枯架柄柊柏柑染柔柚柱柳柴査柿栃栄栖栗校株核根格桂桃案桐桑桜桝桧桶梁梅梓梢梨梯械梶棄棒棚棟森椋植椎検椿楊楓楠楢業楯極楼楽榊榎榛構槌様槙槻樋標模権横樫樹樺樽橋橘機檀櫛欠次欣欧欲欽歌歓止正此武歩歯歳歴死殊残殖段殺殻殿毅母毎毒比毛氏民気水氷永汀汁求汐汗汚江池汰汲決沈沓沖沙沢河油治沼泉泊法波泣泥注泰洋洗洞津洪洲活派流浄浅浜浦浩浪浮浴海消涌涙液涼淀淑淡深淳淵混添清済渉渋渓渕渚減渡渥温測港湊湖湧湯湾湿満源準溜溝溶滅滋滑滝漁漆漏演漢漬潔潜潟潤潮潰澄澤激濃濱瀧瀬灘火灯灰災炉炎炭点為烈烏無焦然焼煙照煮熊熟熱燃燈燕燦燭爆爪父爽爾片版牛牟牧物特犬犯状狂狐狗狩独狭狼猛猪猫献猿獄獅獣玄率玉王玖玲珍珠現球理琉琢琳琴瑚瑛瑞瑠瑳璃環瓜瓦瓶甘甚生産用甫田由甲申男町画界畑畔留畜畝畠略番異畳疾病症痛療発登白百的皆皇皮皿盆益盗盛盟監盤目盲直相省眉看県眞真眠眼着督睦瞬瞳矢知矩短石砂研砲破硫硬碑碧碩確磁磐磨磯礁示礼社祇祈祉祐祖祝神祢祥票祭禁禄禅禎福禰秀私秋科秘秦秩称移稀程税稔稗稚種稲穂積穏穴究空突窓窪立竜章童竪端競竹笑笛笠符第笹筆等筋筑筒答策箇箕算管箱箸節範築篠篤篭簡簾籍米粉粒粕粗粟粥精糖糞糠糸系紀約紅紋納純紗紘紙級素紡索紫細紳紹紺終組経結絡絢給統絵絶絹継続綜維綱網綾綿緋総緑緒線締編緩練縁縄縦縫縮績繁織繰罪置羅羊美群義羽翁習翔翠翼耀老考者耐耕耳耶聖聞聡聴職肇肉肌肝股肥肩育肺背胞胡胤胸能脂脇脈脚脱脳腐腕腫腰腸腹膜膳臣臥臨自臭至致臼興舌舎舘舛舜舞舟航般船艦良色艶芋芙芝芥芦花芳芸芹芽苅苑苔苗若苦苫英茂茄茅茉茜茨茶草荒荘荷荻莉菅菊菌菓菖菜華菱萌萩萱落葉葛葦葵蒔蒲蒸蒼蓋蓑蓬蓮蔦蔭蔵蕗薄薩薫薬薮藁藍藤藻蘇蘭虎虚虫虹虻蚊蛇蛍蛭蜂蜜蝦蝶融螺蟹蟻血衆行術街衛衝衡衣表袋袖被裁裂装裏裕補裟裸製複西要覆覇見規視覚覧親観角解触言計訓託記訪設許訳訴診証評詞詠試詩詰話誉誌認誓誘語誠誤説読課調談請諏論諭諸謙講謝謹識警議譲護讃谷豆豊豚象豪貝貞負財貢貧貨販貫責貯貴買貸費賀賃資賞賢質赤赦走起超越足跡路跳踏身車軌軍軒軟転軸軽載輔輝輪輸辛辞辰農辺辻込迎近返迦迫述迷追退送逃逆透途通速造逢連週進逸遅遊運過道達違遠遣遥適選遺遼避邑那邦邪郁郎郡部郭郵郷都配酒酔酢酸醍醐采釈里重野量金釘釜針釣鈴鉄鉛鉢鉱鉾銀銃銅銘銭鋭鋼錦録鍋鍛鍬鍵鎌鎖鎮鏡鐘鑑長門閉開閑間関閣闇闘阪防阿陀附降限院陣除陰陳陵陶陸険陽隅隆隈隊階随隔際障隠隣隼雀雁雄雅集雑雛離難雨雪雲零雷電震霊霜霞霧露青靖静非面革鞍鞠韓音響頂頃項順須預頓領頭頼題額顔顕願類風飛食飯飲飼飽飾餅養館首香馨馬駄駅駆駐駒駿騎験骨高髪鬼魁魂魅魔魚鮎鮫鮮鯉鯨鳥鳩鳳鳴鴨鴻鵜鶏鶴鷲鷹鷺鹿麓麗麦麻麿黄黒黙鼓鼠鼻齢龍＊\n",
            "num_class: 2287\n",
            "---------------------------------------\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "f:\\Git\\AI-OCR\\venv\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training time:  20.31739568710327\n",
            "[10/30] Train loss: 219.66766, Valid loss: 3.84111, Elapsed_time: 20.32040\n",
            "Current_accuracy : 50.000, Current_norm_ED  : 0.5000\n",
            "Best_accuracy    : 50.000, Best_norm_ED     : 0.5000\n"
          ]
        },
        {
          "ename": "UnboundLocalError",
          "evalue": "local variable 'start' referenced before assignment",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mf:\\Git\\AI-OCR\\EasyOCR_Japanese_20221205.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Git/AI-OCR/EasyOCR_Japanese_20221205.ipynb#ch0000015?line=0'>1</a>\u001b[0m opt \u001b[39m=\u001b[39m get_config(\u001b[39m\"\u001b[39m\u001b[39mconfig_files/ja_filtered_config.yaml\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/Git/AI-OCR/EasyOCR_Japanese_20221205.ipynb#ch0000015?line=1'>2</a>\u001b[0m train(opt, amp\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
            "File \u001b[1;32mf:\\Git\\AI-OCR\\trainer\\train.py:264\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(opt, show_number, amp)\u001b[0m\n\u001b[0;32m    <a href='file:///f%3A/Git/AI-OCR/trainer/train.py?line=259'>260</a>\u001b[0m predicted_result_log \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mdashed_line\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mhead\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mdashed_line\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m    <a href='file:///f%3A/Git/AI-OCR/trainer/train.py?line=261'>262</a>\u001b[0m \u001b[39m#show_number = min(show_number, len(labels))\u001b[39;00m\n\u001b[1;32m--> <a href='file:///f%3A/Git/AI-OCR/trainer/train.py?line=263'>264</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(labels), \u001b[39m'\u001b[39m\u001b[39mshow_numbers \u001b[39m\u001b[39m'\u001b[39m, show_number, \u001b[39m'\u001b[39m\u001b[39mstart \u001b[39m\u001b[39m'\u001b[39m, start)   \n\u001b[0;32m    <a href='file:///f%3A/Git/AI-OCR/trainer/train.py?line=264'>265</a>\u001b[0m start \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m,\u001b[39mlen\u001b[39m(labels) \u001b[39m-\u001b[39m show_number ) \n\u001b[0;32m    <a href='file:///f%3A/Git/AI-OCR/trainer/train.py?line=265'>266</a>\u001b[0m \u001b[39mfor\u001b[39;00m gt, pred, confidence \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(labels[start:start\u001b[39m+\u001b[39mshow_number], preds[start:start\u001b[39m+\u001b[39mshow_number], confidence_score[start:start\u001b[39m+\u001b[39mshow_number]):\n",
            "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'start' referenced before assignment"
          ]
        }
      ],
      "source": [
        "opt = get_config(\"config_files/ja_filtered_config.yaml\")\n",
        "train(opt, amp=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AxHaq1rgGVf"
      },
      "source": [
        "## Loading the Trained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjiUr1Jns-GH"
      },
      "source": [
        "Copy custom_model.py custom_model.yaml  to the root directory: /root/EasyOCR/user_network/\n",
        "\n",
        "copy custom_model.pth to /root/EasyOCR/model/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuCxJZOzm4Ef"
      },
      "outputs": [],
      "source": [
        "!cp /root/EasyOCR/user_network/custom_model.yaml /root/.EasyOCR/user_network/custom_model.yaml\n",
        "!cp /root/EasyOCR/user_network/custom_model.py /root/.EasyOCR/user_network/custom_model.py\n",
        "!cp /root/EasyOCR/model/custom_model.pth /root/.EasyOCR/model/custom_model.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LO5lnLgjtWb7"
      },
      "outputs": [],
      "source": [
        "import easyocr\n",
        "reader = easyocr.Reader(['ja'], recog_network='custom_model')\n",
        "result = reader.readtext('/content/EasyOCR/trainer/all_data/ja_val/tate001_20211203101858_00001_page_1_character_102_0.tif', detail = 0)  # remove detail =0 for details.. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cs2SH_yJG81i",
        "outputId": "5eae9c26-28ba-4009-bf8d-17370c5d15a5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['獅', '秀', '腫', '腫', '物']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result = reader.readtext('/content/name.jpg', detail = 0)  # remove detail =0 for details.. \n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NbFmPF7GTT6",
        "outputId": "9d700f6e-1ff5-42cd-9d2d-ec7d620467fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imagecodecs in /usr/local/lib/python3.7/dist-packages (2021.11.20)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from imagecodecs) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install tifffile \n",
        "!pip install imagecodecs\n",
        "!pip install imagecodecs-lite"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "EasyOCR_Japanese_20220305.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "1e2fa7ef4728c44df11d01ab1501385da2466265e595d463c38440777f4d34b7"
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
